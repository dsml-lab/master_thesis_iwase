\chapter{実験}
\label{chap:experiment}

本章では，第\ref{chap:experiment_settings}章で提案したデータセットを用いた，実験方法を述べる．
本研究の目的は，色と数字の概念がどのように獲得されるかを学習過程から分析することである．この目的を達成するために，100クラス分類タスクを設定し，訓練データとテストデータにおける詳細なエラー率を観察した．

訓練データに対するエラー率の観察では，以下の条件を設定した（表\ref{tab:train_error}）．色と数字の両方を含む概念，色のみの概念，数字のみの概念に対して，それぞれラベルノイズがある場合とない場合の分類エラー率を記録した．

\begin{table}[h]
\centering
\caption{訓練データに対するエラー率の観察条件}
\begin{tabular}{lll}
\toprule
\textbf{概念} & \textbf{ラベルノイズの有無} & \textbf{分類タスク} \\
\midrule
色＋数字 & 有・無 & 100クラス分類 \\
色のみ   & 有・無 & 色分類タスク \\
数字のみ & 有・無 & 数字分類タスク \\
\bottomrule
\end{tabular}
\label{tab:train_error}
\end{table}

テストデータに対するエラー率の観察では，ラベルノイズがない条件で，色と数字の両方を含む概念，色のみの概念，数字のみの概念に対するエラー率を記録した（表\ref{tab:test_error}）．

\begin{table}[h]
\centering
\caption{テストデータに対するエラー率の観察条件}
\begin{tabular}{lll}
\toprule
\textbf{概念} & \textbf{ラベルノイズの有無} & \textbf{分類タスク} \\
\midrule
色＋数字 & 無 & 100クラス分類 \\
色のみ   & 無 & 色分類タスク \\
数字のみ & 無 & 数字分類タスク \\
\bottomrule
\end{tabular}
\label{tab:test_error}
\end{table}

これらの設定を通じて，色と数字の概念が獲得される過程と，ラベルノイズの有無が分類性能に与える影響について詳細に分析することを目指した．

\section{実験設定}
\label{sec:experiment_settings}
実験設定において，提案したデータセットに基づき，小程度から大程度までのCNNモデルを用いて学習を行う．
今回の実験においては，5層のCNN，8層のCNN，そして，VGGNetを使って，学習過程を観察した．
今回は，訓練データとテストデータに関して，以下の学習曲線を観察し，詳細な学習過程の分析を行う．その観察した学習曲線は，以下の通りである．


また，5層と8層のモデルにおいて，Model Widthを変化させ，モデルキャパシティを変更したときの学習過程を観察した．

以下に詳細な実験設定を示す．

\begin{table}[ht]
    \centering
    \caption{モデル設定}
    \begin{tabular}{ll}
        \midrule
        Model Layers & 5, 8, VGG \\
        Model Width & 1, 2, 4, 8 \\
        \bottomrule
    \end{tabular}
    \label{tab:model_settings}
\end{table}

また，これらの学習のパラメータを以下に示す．

\begin{table}[ht]
    \centering
    \caption{学習設定}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Values} \\
        \midrule
        Epochs & 1000 \\
        Batch Size & 256 \\
        Learning Rate & 0.01 \\
        Optimizer & SGD \\
        Momentum & 0.0 \\
        Loss Function & Cross Entropy \\
        \bottomrule
    \end{tabular}
    \label{tab:training_settings}
\end{table}

これらのCNNモデルの構造を表\ref{tab:cnn5layer}，表\ref{tab:cnn8layer}，表\ref{tab:vgg16}に示す．
Width Multiplierは，モデルの幅を調整するためのパラメータである．

\begin{table}[h]
    \centering
    \caption{5層のCNNのアーキテクチャ}
    \label{tab:cnn5layer}
    \begin{tabular}{lll}
    \toprule
    \textbf{Layer} & \textbf{Output Shape} & \textbf{Details} \\
    \midrule
    Input & $3 \times 32 \times 32$ & RGB image \\
    Conv1 & $16 \cdot \text{width multiplier} \times 32 \times 32$ & $3 \times 3$ kernel, stride 1, padding 1 \\
    ReLU1 & Same as above & Activation \\
    MaxPool1 & $16 \cdot \text{width multiplier} \times 16 \times 16$ & $2 \times 2$ kernel, stride 2 \\
    Conv2 & $32 \cdot \text{width multiplier} \times 16 \times 16$ & $3 \times 3$ kernel, stride 1, padding 1 \\
    ReLU2 & Same as above & Activation \\
    MaxPool2 & $32 \cdot \text{width multiplier} \times 8 \times 8$ & $2 \times 2$ kernel, stride 2 \\
    Conv3 & $64 \cdot \text{width multiplier} \times 8 \times 8$ & $3 \times 3$ kernel, stride 1, padding 1 \\
    ReLU3 & Same as above & Activation \\
    Conv4 & $128 \cdot \text{width multiplier} \times 8 \times 8$ & $3 \times 3$ kernel, stride 1, padding 1 \\
    ReLU4 & Same as above & Activation \\
    MaxPool4 & $128 \cdot \text{width multiplier} \times 4 \times 4$ & $2 \times 2$ kernel, stride 2 \\
    Conv5 & $256 \cdot \text{width multiplier} \times 4 \times 4$ & $3 \times 3$ kernel, stride 1, padding 1 \\
    ReLU5 & Same as above & Activation \\
    Fully Connected & $256 \cdot (\text{img size}/8)^2 \times 10$ & Output: 10 classes \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{8層のCNNのアーキテクチャ}
    \label{tab:cnn8layer}
    \begin{tabular}{lll}
    \toprule
    \textbf{Layer} & \textbf{Output Shape} & \textbf{Details} \\
    \midrule
    Same as CNN5Layer up to Conv5 & & \\
    Conv6 & $512 \cdot \text{width multiplier} \times 4 \times 4$ & $3 \times 3$ kernel, stride 1, padding 1 \\
    ReLU6 & Same as above & Activation \\
    MaxPool6 & $512 \cdot \text{width multiplier} \times 2 \times 2$ & $2 \times 2$ kernel, stride 2 \\
    Conv7 & $512 \cdot \text{width multiplier} \times 2 \times 2$ & $3 \times 3$ kernel, stride 1, padding 1 \\
    ReLU7 & Same as above & Activation \\
    Conv8 & $512 \cdot \text{width multiplier} \times 2 \times 2$ & $3 \times 3$ kernel, stride 1, padding 1 \\
    ReLU8 & Same as above & Activation \\
    Fully Connected & $512 \cdot (\text{img size}/16)^2 \times 10$ & Output: 10 classes \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{VGG16のアーキテクチャ}
    \label{tab:vgg16}
    \begin{tabular}{lll}
    \toprule
    \textbf{Layer} & \textbf{Output Shape} & \textbf{Details} \\
    \midrule
    Feature Extractor & $512 \cdot (\text{img size}/32)^2$ & Based on VGG16 configuration \\
    Adaptive Pooling & $512 \cdot 7 \times 7$ & Output size fixed to $7 \times 7$ \\
    Fully Connected & & \\
    FC1 & $4096 \cdot \text{width multiplier}$ & Fully connected, ReLU, Dropout \\
    FC2 & $4096 \cdot \text{width multiplier}$ & Fully connected, ReLU, Dropout \\
    FC3 & $10$ & Fully connected, Output: 10 classes \\
    \bottomrule
    \end{tabular}
\end{table}
