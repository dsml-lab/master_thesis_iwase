\chapter{先行研究}
\section{深層学習}
一般に，機械学習で使用されるモデルは決定木，サポートベクターマシン（SVM），ニューラルネットワークなどが存在する．決定木は得られた予測に対して，どの説明変数が影響したのかの判断が容易であり，説明可能性が高いことで知られている．
一方で，ニューラルネットワークは，パーセプトロンを筆頭に，層の増加やネットワークの複雑化が図られてきた．黎明期においては，非線形な問題をとけるように知見が盛り込まれたSVMや，生物が持つ視覚野の知見から提案されたネオコグニトロンなどの画期的な手法が提案されてきた．その中でも，ネオコグニトロンに端を発する，畳み込みニューラルネットワーク(CNN)は，LeNet\cite{LeNet}により，誤差逆伝播法が導入され，2010年代以降には，AlexNet\cite{AlexNet}，VGGNet\cite{VGGNet}，ResNet\cite{He:ResNet}，と急速に進化を遂げてきた．
このような深層化されたニューラルネットワークは興味深い性質や振る舞いを示す．しかし，そのような性質がどのような機序によって引き起こされるかについての完全な合意はとられていない．

\section{二重降下現象}
機械学習において，モデルの性能はモデルの複雑性（例えば，パラメータ数）と深い関係があり，モデルのパラメータ数が不足することによるアンダーフィッティング（Underfitting）\cite{underfitting}や，過剰なパラメータによるオーバーフィッティング（Overfitting）\cite{overfitting}などの現象が知られている．モデルの複雑性が増すにつれて，初めは性能が向上し（アンダーフィッティングを克服），その後過剰な複雑性により性能が低下するとされていた．これはU字型のカーブ，いわゆるバイアス-バリアンス トレードオフ\cite{Rajnarayan2010}として知られている．

ところが近年発見されたDouble Descent\cite{Belkin_2019}と呼ばれている現象は，モデルの複雑性がさらに増すと，性能が再び向上する．つまり，最初のU字型のカーブ（アンダーフィッティングからオーバーフィッティングへの移行）の後，さらに複雑性が増加すると，新たな性能向上のフェーズが現れるのである．過剰パラメータを持つディープニューラルネットワークが，理論的にはオーバーフィッティングを起こすべきなのに，実際には優れた汎化性能を示す場合がある\cite{He:ResNet,ViT}．

このDouble Descentは，Belkinら\cite{Belkin_2019}によって決定木や二層のニューラルネットワークで確認され，その後，Nakkiranら\cite{nakkiran2021deep}が，ディープニューラルネットワーク（DNN）においても観察されること，学習エポック数の増加に対してもDouble Descentが起こることを示した．さらに，パラメータの枝刈りによるスパース性の増加に対してもDouble Descentが起こることが報告されている\cite{He}．パラメータ数，学習エポック数，スパース性の増加に伴って観察されるDouble Descentは，それぞれ，Model-wise Double Descent，Epoch-wise Double Descent，Sparse Double Descent と呼ばれている\cite{nakkiran2021deep,He}．

\subsection{Model-wise double descent}
Yangらは，バイアス-分散のトレードオフに関する古典的な理論を，広範な実験を通して再検討した\cite{Yang}．彼らは，分類理論が予測するようにバイアスが単調減少する一方で，分散は単峰性の挙動を示すことを発見した．このバイアスと分散の組み合わせは，3つの典型的なリスクカーブパターンを示唆しており，すでに報告されている多くの実験結果と一致している．また，Somepalliらは，新たな決定境界可視化手法を提案し，二重降下におけるエラーの悪化する領域において，決定境界が断片化していることを報告している\cite{Somepalli}．
さらに，Curthらは決定木などのBelkinらが二重降下を観察した条件において，2つの次元の軸によるU字のカーブの重ね合わせによって二重降下が起きると報告し，深層学習における二重降下においても，この観点は良い指針になることを示唆している\cite{Curth_NeurIPS2023}．

\subsection{Epoch-wise double descent}
統計的シミュレーションの結果から，学習過程における二重降下に関するいくつかの仮説が浮かび上がってきた．これらの仮説はデータの特徴に焦点を当てている．例えば，Stephensonらは，二重降下は遅いが有益な特徴によって起こると仮定し，理想的な線形モデルにおいてデータの主成分を除去することで二重降下の挙動を除去できることを示している\cite{Stephenson}．一方，Pezeshkiらは実験により，異なる時間スケールで学習された特徴が二重降下を引き起こすことを発見している\cite{Pezeshki}．さらに，Heckelらは，モデルの異なる部分が異なるエポックで学習することによる，複数のバイアスと分散の重複トレードオフが二重降下を引き起こすとしている\cite{Heckel}．そのうえで，層間で学習率を変えることで二重降下を緩和できることを実証している．

\subsection{Sparse double descent}
モデルのスパース性が高まるにつれて，つまり多くのパラメータがゼロまたは非常に小さくなるにつれ，まず性能の向上が見られる~\cite{He, SDD_VIT}．しかし，ある点を境に性能は低下する．さらにスパース性を高めると，性能は再び向上する．このことは，ネットワークの刈り込みのような方法で達成可能な適度なスパース性が，モデルのオーバーフィッティングを抑制し，汎化性能を向上させることを示唆している．また，枝刈り前のモデルの大きさに関わらず，枝刈り後の精度は一定である可能性が示唆されている~\cite{Arora_SDD}．

\section{画像認識における形状・テクスチャ}
Geirhosらは，ImageNetで学習したCNNが，分類のために特に画像のテクスチャを重視することを示した~\cite{Geirhos}．彼らは，相反する形状とテクスチャ情報を持つ画像をCNNに入力し，出力が形状ベースのラベルとテクスチャベースのラベルのどちらに一致するかをチェックした．この結果に基づいて，CNNが認識において形状とテクスチャのどちらを優先するかを分析した．一方，Islamらは，ニューロンの潜在表現に基づくモデルにおいて，形状とテクスチャのどちらを重視するかを定量的に判断する方法を提案した~\cite{Islam}．この方法によって，CNNがどの特徴に偏重するのかを定量的に分析することができる．さらに，Geらは人間の視覚系のモデル化を試み，Human Vision System (HVS)を開発した．HVSは，画像分類時にどの特徴（形状，テクスチャ，色など）が最も重要な役割を果たすかを定量的に評価可能である\cite{Ge}．

本研究は，画像理解と二重降下における CNN に関する先行研究を基礎としている．Islamらの手法を使用して，CNN学習中のテクスチャと形状情報に関する知識の獲得と二重降下現象との関係を明らかにすることを試みた．Islamらの手法を利用し，最終畳み込み層における形状・テクスチャそれぞれをエンコードするニューロン数を推定，推定した割合の値を形状・テクスチャ偏重度としている．
\newpage