\chapter{先行研究}
\section{深層学習}
一般に，機械学習で使用されるモデルは決定木，サポートベクターマシン（SVM）\cite{SVM}，ニューラルネットワークなどが存在する．
決定木は得られた予測に対して，どの説明変数が影響したのかの判断が容易であり，説明可能性が高いことで知られている．
一方で，ニューラルネットワークは，パーセプトロンを筆頭に，層の増加やネットワークの複雑化が図られてきた．
黎明期においては，非線形な問題をとけるように知見が盛り込まれたSVMや，生物が持つ視覚野の知見から提案されたネオコグニトロンなどの画期的な手法が提案されてきた．
その中でも，ネオコグニトロンに端を発する，畳み込みニューラルネットワーク(CNN)は，LeNet \cite{LeNet}により，誤差逆伝播法が導入され，2010年代以降には，AlexNet\cite{AlexNet}，VGGNet\cite{VGGNet}，ResNet\cite{ResNet}，と急速に進化を遂げてきた．
このような深層化されたニューラルネットワークは興味深い性質や振る舞いを示す．しかし，そのような性質がどのような機序によって引き起こされるかについての完全な合意はとられていない．

\newpage

\section{二重降下現象}
機械学習において，モデルの性能はモデルの複雑性（例えば，パラメータ数）と深い関係があり，モデルのパラメータ数が不足することによるアンダーフィッティング（Underfitting）や，
過剰なパラメータによるオーバーフィッティング（Overfitting）などの現象が知られている．モデルの複雑性が増すにつれて，初めは性能が向上し（アンダーフィッティングを克服），
その後過剰な複雑性により性能が低下するとされていた．これはU字型のカーブ，いわゆるバイアス-バリアンストレードオフ\cite{Rajnarayan2017-ml,Belkin2018-yx}として知られている．

ところが近年発見されたDouble Descent(二重降下現象)\cite{Belkin_2019}と呼ばれている現象は，モデルの複雑性がさらに増すと，
性能が再び向上する．つまり，最初のU字型のカーブ（アンダーフィッティングからオーバーフィッティングへの移行）の後，
さらに複雑性が増加すると，新たな性能向上のフェーズが現れるのである．過剰パラメータを持つディープニューラルネットワークが，
理論的にはオーバーフィッティングを起こすべきなのに，実際には優れた汎化性能を示す場合がある\cite{ResNet, ViT}．

このDouble Descentは，Belkinら\cite{Belkin_2019}によって決定木や二層のニューラルネットワークで確認され，
その後，Nakkiranら\cite{Nakkiran2021}が，ディープニューラルネットワーク（DNN）においても観察されること，
学習エポック数の増加に対してもDouble Descentが起こることを示した．
さらに，パラメータの枝刈りによるスパース性の増加に対してもDouble Descentが起こることが報告されている\cite{He2016-et}．
パラメータ数，学習エポック数，スパース性の増加に伴って観察されるDouble Descentは，
それぞれ，Model-wise Double Descent，Epoch-wise Double Descent，Sparse Double Descent と呼ばれている\cite{Nakkiran2021, He2016-et}．

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/doubledescent.pdf}
    \caption[二重降下現象の概念図]{二重降下現象の概念図．モデルの複雑性が増すにつれて，性能が向上し，その後性能が低下するU字型のカーブ（バイアス-バリアンス トレードオフ）の後，さらに複雑性が増加すると性能が再び向上する二重降下現象が観測される．}
    \label{fig:DoubleDescent}
\end{figure}

\section{画像認識における形状・テクスチャ}
Geirhosらは，ImageNetで学習したCNNが，分類のために特に画像のテクスチャを重視することを示した\cite{Geirhos}．
彼らは，形状とテクスチャ情報を持つ画像をCNNに入力し，その出力が形状ベースのラベルとテクスチャベースのラベルのどちらに一致するかを調査した．
一方，Islamらは，ニューロンの潜在表現に基づくモデルにおいて，形状とテクスチャのどちらを重視するかを定量的に判断する方法を提案した\cite{}．
この方法によって，CNNがどの特徴に偏重するのかを定量的に分析することができる．この定量的に計算する手法の概要を
図\ref{fig:IslamSTB}に示す．また，使用した画像例を図\ref{fig:esserset}に示す．
さらに，Geらは人間の視覚系のモデル化を試み，Human Vision System (HVS)を開発した．
HVSは，画像分類時にどの特徴（形状，テクスチャ，色など）が最も重要な役割を果たすかを定量的に評価可能である\cite{Ge}．

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/shapetexturebias.pdf}
    \caption[形状バイアスとテクスチャバイアスの概念図]{形状バイアスとテクスチャバイアスの概念図．Aの画像は人間，CNNどちらもテクスチャ情報から象と認識できる．また，Bの画像も猫と認識できる．
    しかし，Cの猫の画像に象のテクスチャを重ね合わせた場合，人間は猫と判断するが，CNNは象と分類する．}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/IslamSTB.pdf}
    \caption[形状バイアスとテクスチャバイアスの計算手法]{形状バイアスとテクスチャバイアスの計算手法．
    最終畳み込み層において，形状およびテクスチャの特徴に反応するニューロンの割合を作成したデータセットから計算する．}
    \label{fig:IslamSTB}
\end{figure}

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.19\linewidth}
        \includegraphics[width=\linewidth]{fig/islam_dataset/islam_1.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\linewidth}
        \includegraphics[width=\linewidth]{fig/islam_dataset/islam_2.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\linewidth}
        \includegraphics[width=\linewidth]{fig/islam_dataset/islam_3.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\linewidth}
        \includegraphics[width=\linewidth]{fig/islam_dataset/islam_4.pdf}
    \end{subfigure}
    \begin{subfigure}[b]{0.19\linewidth}
        \includegraphics[width=\linewidth]{fig/islam_dataset/islam_5.pdf}
    \end{subfigure}
    \caption[Islamらの提案した形状・テクスチャバイアスの計算手法時に使用する画像例]{Islamらの提案した形状・テクスチャバイアスの計算手法時に使用する画像例．
    同じ形状（飛行機）に異なるテクスチャを被せている．}
    \label{fig:esserset}
\end{figure}

\newpage

\section{画像認識における二重降下現象と形状・テクスチャバイアスの関係}

\UTF{9AD9}橋らの研究\cite{DD_STB}は，画像認識タスクにおける二重降下現象と，CNNの形状バイアスおよびテクスチャバイアスの変化の関連性を示唆した．
この研究では，二重降下現象と形状・テクスチャバイアスの変化のタイミングに相関が見られ，
テスト誤り率が上昇から下降に転じるタイミングと，形状バイアスが増加し始めるタイミングが一致することが示された．
また，事前学習の有無による学習過程の違いも観測された．事前学習ありのモデルでは初期からテクスチャバイアスが高く，学習が進むと形状バイアスが増加する傾向が見られた．一方，事前学習なしのモデルでは学習初期は形状バイアスが高く，その後テクスチャバイアスが増加する傾向が確認された．

岩瀬らの研究~\cite{icpr2024iwase}では，さらに形状・テクスチャバイアスと二重降下現象に関する同期性を様々な角度から検証している．この研究では，CIFAR-10とResNet18の組み合わせ以外の条件下で同期性が確認されたほか，どの層が要因となり，同期性が起こっているのかが明らかとなった．
これらの知見は，画像認識モデルの学習ダイナミクスと二重降下現象の関係性に新たな洞察を提供している．
この同期性を図\ref{fig:iwaseICPR}に示す．

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{fig/iwaseICPR.pdf}
    \caption[ResNet18の学習過程における二重降下現象と形状・テクスチャバイアスの変化の同期性]{ResNet18の学習過程における二重降下現象と形状・テクスチャバイアスの変化の同期性．
    上図は，ImageNetで事前学習済みのResNet18を使用し，CIFAR-10とCIFAR-100の学習を行った際のテストエラー率．
    また，下図はそれぞれのエポックでのモデルにおける形状・テクスチャバイアスを計算した結果．
    二重降下現象を3つのフェーズに分けた際に，第1フェーズでは，テストエラー率が減少するときにはテクスチャバイアスが強くなる．
    次に過学習のタイミングで形状バイアスが強くなり，再度はエラー率が下がるときテクスチャバイアスを強くするように戻る．}
    \label{fig:iwaseICPR}
\end{figure}

\section{CNNモデルにおける汎化性能の解析}
機械学習の分野において，汎化性能の解析は重要な研究課題である．
CNNモデルの汎化性能に関して，特徴マップの可視化\cite{DBLP:conf/cvpr/ZeilerF14}や内部表現のクラスタリングなど，多角的な評価指標を用いた解析が行われている．
また，モデルのアンサンブルやドロップアウトなどの手法により，汎化性能を向上させる研究も進められている\cite{JMLR:v15:srivastava14a}．
これらの手法は過学習を防ぎ，モデルの汎化性能を高めるのに有効であることが示されているものの，そのメカニズムの解明には課題が残されている．
近年，CNNモデルの汎化性能における非直感的な挙動の解明に向けた研究が進展している．Pezeshkiら\cite{Pezeshki22}は，
二重降下現象に着目し，学習中の特徴が異なるスケールで学習されるプロセスがモデルの複雑性と汎化性能に与える影響を明らかにした．
また，Jeffaresら\cite{Jeffares2024}は線形近似モデルを用いて，学習プロセス中の中間的な更新が汎化性能に与える影響を調査し，
二重降下現象やGrokking現象の発生メカニズムを定量的に解析している．
さらに，Arpitら\cite{Arpit2017}は，CNNモデルが単純なパターンの学習からノイズの記憶へと段階的に移行することを実験的に示した．
特に，クリティカルサンプル比率（CSR）を用いた解析により，ノイズデータの学習には複雑な決定境界が必要であることが明らかになった．
これらの知見は，モデルの汎化性能を理解する上で，パラメータ数や複雑性だけでなく，学習過程全体を通じた特徴量の進化やデータの特性を考慮する重要性を示唆している．
